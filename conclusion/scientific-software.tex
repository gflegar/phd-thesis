\section{Designing Scientific Software for Sparse Computations}

With the previous chapters of this work focusing on the components for the
solution of linear systems, the natural next consideration is the integration of
these components into a complete software for the solution of linear systems.
The first obstacle is the endless amount of combinations in which distinct
components can be combined. Each matrix format can be used with any Krylov
method. On top of that, each combination can be enhanced with any
preconditioner, which could be available directly as a matrix provided by the
user (stored in one of the formats supported by the software, or even in a
custom format), generated from the system matrix as one of the standard
preconditioners (\eg, relaxation or factorization based), or even implemented as
a coarse (Krylov) solver. The system matrix may not even be stored explicitly,
but available as a specialized implementation provided by the user. The
preconditioners themselves may also be complex methods for the solution of
linear systems, as even the most common ILU-based preconditioners can be
constructed by selecting components from a pool of factorization methods that
generate a preconditioner, and a pool of linear system solution methods that are
used to solve systems with the upper and lower triangular factors generated by
the factorization. Another issue that increases the complexity of software
design is the heterogeneity of modern hardware. If the goal of the software is
to support computation on various devices, or even collaborative computation on
a heterogeneous platform, concepts defining the hardware resources that are used
to perform the computations also have to be added to the design.

While there are various high performance libraries that try to address these
issues~\cite{magma,vienna-cl,paralution}, they usually lack at least one of the
points mentioned above, or are somewhat difficult to use. This section proposes
one possible theoretical design aimed at solving these issues and briefly
describes a new library that is based on it.

\subsection{Matrices}
\label{conclusion:ssec:matrices}

The ultimate tool of reducing complexity in both mathematics and software
engineering is abstraction. By hiding all the unnecessary details of a
mathematical or software object, and only representing it as a concept with
certain operations that behave according to some rules, every concrete object
that defines these operations and follows the same rules as the concept can be
handled in exactly the same way. In the context of Krylov methods, matrices are
used only as part of the matrix-vector product operation $y = Ax$. In rare
cases, the conjugate transpose matrix (\textdef{adjoint matrix}) $A^*$ is also
needed as part of the conjugate transpose matrix-vector product operation $y =
A^*x$. For a fixed matrix $A$, the key properties of the matrix-vector product
are additivity ($A(x+y) = Ax + Ay$) and homogeneity ($A(\alpha x) = \alpha Ax$).
Focusing only on these properties allows us to define an abstraction that can
hide the details about the coefficients and the storage scheme of the matrix.

\begin{theorem}
\label{conclusion:thm:matrix-linop}
For any matrix $A \in \mathbb{F}^{m \times n}$, let $L_A : \mathbb{F}^n
\rightarrow \mathbb{F}^m$ be an operator that satisfies $L_Ax = Ax$, $x \in
\mathbb{F}^n$. The following holds:
\begin{enumerate}
\item $L_A$ is a linear operator, \ie $L_A(\alpha x + \beta y) = \alpha L_A x +
\beta L_A y$, for any $\alpha,\beta \in \mathbb{F}$ and $x, y \in \mathbb{F}^n$.
\item $L_A$ is unique, \ie, if $L_A \neq L_B$ then $A \neq B$, for any $A, B
\in \mathbb{F}^{m \times n}$.
\item The mapping $A \mapsto L_A$ is injective, \ie, if $L_A = L_B$ then $A
= B$, for any $A, B \in \mathbb{F}^{m \times n}$.
\item The mapping $A \mapsto L_A$ is surjective onto the set of linear operators
\begin{align}
\mathcal{L}(\mathbb{F}^n,\mathbb{F}^m) := \lbrace L : \mathbb{F}^n \rightarrow
\mathbb{F}^m \ \vert \ L(\alpha x + \beta y) = \alpha Lx + \beta Ly, \ \forall
\alpha, \beta \in \mathbb{F}, \forall x, y \in \mathbb{F}^n \rbrace,
\end{align}
\ie, for every $L \in \mathcal{L}(\mathbb{F}^n,\mathbb{F}^m)$ there is a
matrix $A \in \mathbb{F}^{m \times n}$ such that $L = L_A$.
\end{enumerate}
Thus, $A \mapsto L_A$ defines a one-to-one correspondence between the set of
matrices $\mathbb{F}^{m \times n}$ and linear operators
$\mathcal{L}(\mathbb{F}^n, \mathbb{F}^m)$.
\end{theorem}
\begin{proof}~\\
\begin{enumerate}
\item This is a trivial consequence of the definition of $L_A$: $L_A(\alpha x +
\beta y) = A(\alpha x + \beta y) = \alpha Ax + \beta Ay = \alpha L_Ax + \beta
L_Ay$.
\item If $L_A \neq L_B$, then there exists a vector $x$ such that $Ax = L_Ax
\neq L_B x = Bx$, which implies that $(A - B)x \neq 0$. This is only possible if
$A - B \neq 0$, \ie $A \neq B$.
\item For $i = 1,2, \ldots, n$, let $a_i$ and $b_i$ denote the $i$-th column of
matrices $A$ and $B$, respectively, and let $e_i \in \mathbb{F}^n$ be the $i$-th
vector of the standard basis for $\mathbb{F}^n$, \ie, the $i$-th component of
$e_i$ is $1$, while its every other component is $0$. Then, $a_i = Ae_i = L_Ae_i
= L_B e_i = Be_i = b_i$, for every column $i$, which implies $A = B$.
\item For any linear operator $L \in \mathcal{L}(\mathbb{F}^n, \mathbb{F}^m)$ we
construct the matrix $A \in \mathbb{F}^{m \times n}$ by defining its $i$-th
column as $a_i := Le_i$. Then, for any vector $x = [ x_1, x_2, \ldots, x_n ]^T 
\in \mathbb{F}^n$, the following holds:
\begin{align}
L_Ax &= Ax = A(\sum_{i=1}^n x_i e_i) j = \sum_{i=1}^n x_i Ae_i \nonumber \\
     &= \sum_{i=1}^n x_i a_i = \sum_{i=1}^n x_i Le_i
      = L(\sum_{i=1}^n x_i e_i) \label{conclusion:eqn:surjection} \\
     &= Lx, \nonumber
\end{align}
generating the equality $L_A = L$.
\end{enumerate}
\end{proof}

\begin{definition}
For any matrix $A \in \mathbb{F}^{m \times n}$, the unique linear operator $L_A
\in \mathcal{L}(\mathbb{F}^n, \mathbb{F}^m)$ defined by $L_Ax := Ax$ for $x \in
\mathbb{F}^n$ is called the \textdef{matrix product operator} with respect
to $A$.
\end{definition}

Theorem~\ref{conclusion:thm:matrix-linop} shows that every matrix can be
uniquely represented by its matrix product operator, and that a specific matrix
can be reconstructed given its linear operator representation. However, to
effectively replace matrices with linear operators, the structure of algebraic
operations defined on matrices has to be mimicked by linear operators.

\begin{definition}
Let $L \in \mathcal{L}(\mathbb{F}^n, \mathbb{F}^m)$ be a linear operator and
let $(\cdot, \cdot)_m : \mathbb{F}^m \times \mathbb{F}^m \rightarrow
\mathbb{F}$ and $(\cdot, \cdot)_n : \mathbb{F}^n \times \mathbb{F}^n
\rightarrow \mathbb{F}$ be any two inner products~\cite{la} on vector spaces
$\mathbb{F}^m$ and $\mathbb{F}^n$, respectively. A linear operator $K \in
\mathcal{L}(\mathbb{F}^m, \mathbb{F}^n)$ that satisfies
\begin{align}
    (Lx, y)_m = (x, Ky)_n \label{conclusion:eqn:adjoint}
\end{align}
for every $x \in \mathbb{F}^n$ and $y \in \mathbb{F}^m$ is called an
\textdef{adjoint operator} of $L$ with respect to inner products $(\cdot,
\cdot)_m$ and $(\cdot, \cdot)_n$, and is denoted by $L^*$.
\end{definition}

\begin{theorem}
\label{conclusion:thm:adjoint}
For any matrix $A \in \mathbb{F}^{m \times n}$, the adjoint operator $L_A^*$ of
the matrix product operator $L_A$ with respect to inner products $(x, y)_m :=
x^*y, \ \forall x,y \in \mathbb{F}^m$ and $(w, z)_n := w^*z, \ \forall w, z \in
\mathbb{F}^n$ is unique, and $L_A^* = L_{A^*}^{}$.
\end{theorem}

\begin{proof}
Let $K$ be any adjoint operator for $L_A$. Then, applying
Equation~(\ref{conclusion:eqn:adjoint}) on $e_i \in \mathbb{F}^n$ and $e_j \in
\mathbb{F}^m$ yields:
\begin{align}
    (Ke_j)_i &= e_i^*Ke_j^{} = (e_i, Ke_j)_n = (L_Ae_i, e_j)_m \nonumber \\
             &= (Ae_i,  e_j)_m = (a_i, e_j)_m =a_i^*e_j^{} = \overline{a_{ji}}
             \\
             &= (A^*)_{ij} \nonumber
\end{align}
The equality $K = L_{A^*}$ is obtained by applying
Equation~(\ref{conclusion:eqn:surjection}) to $L_{A^*}$ and $K$.
\end{proof}

\begin{corollary}
The mapping $A \mapsto L_A$ is an isomorphism between the set of
matrices $F^{m \times n}$ and linear operators $\mathcal{L}(\mathbb{F}^n,
\mathbb{F}^m)$ that preserves the algebraic structure of the matrix-vector
multiplication and conjugate transpose operations, that is:
\begin{enumerate}
\item Matrix-vector multiplication is equivalent to the application of the
matrix product operator: $Ax = L_Ax$, for $x \in \mathbb{F}^n$.
\item Conjugate transpose matrix-vector multiplication is equivalent to the
application of the matrix product operator's adjoint: $A^*x = L_A^*x$, for $x
\in \mathbb{F}^n$.
\end{enumerate}
\end{corollary}

\begin{proof}
Relation 1 is true by the definition of operator $L_A$. Relation 2 is a direct
consequence of Relation 1 and Theorem~\ref{conclusion:thm:adjoint}.
\end{proof}

\subsection{Linear Systems}
\label{conclusion:ssec:systems}

Another operation that a Krylov solver-focused library has to implement is,
obviously, the solution of a linear system. Interestingly, this operation
satisfies the same additivity and homogeneity properties as the matrix-vector
multiplication.

\begin{definition}
\label{conclusion:def:solver-linop}
For any nonsingular matrix $A \in GL_n(\mathbb{F}) := \lbrace M \in \mathbb{F}^n
\ \vert \ \det(M) \neq 0 \rbrace$, let $S_A : \mathbb{F}^n \rightarrow
\mathbb{F}^n$ be an operator that satisfies $A(S_Ab) = b$, for $b \in
\mathbb{F}^n$. That is, the operator $S_A$ maps any vector $b$ into the solution
of the linear system with the system matrix $A$ and the right hand side $b$.
Such operator is called a \textdef{solver operator} with respect to $A$.
\end{definition}

\begin{theorem}
\label{conclusion:thm:solver-linop}
For any $A \in GL_n(\mathbb{F})$:
\begin{enumerate}
\item $S_A$ is a linear operator, \ie $S_A(\alpha x + \beta y) = \alpha S_A x +
\beta S_A y$, for any $\alpha,\beta \in \mathbb{F}$ and $x, y \in \mathbb{F}^n$.
\item $S_A$ is invertible and $S_A^{-1} = L_A^{}$.
\item $S_A$ is unique, \ie, if $S_A \neq S_B$ then $A \neq B$, for any $A, B
\in GL_n(\mathbb{F})$.
\item The mapping $A \mapsto S_A$ is injective, \ie, if $S_A = S_B$ then $A
= B$, for any $A, B \in GL_n(\mathbb{F})$.
\item The mapping $A \mapsto S_A$ is surjective onto the set
\begin{align}
\aut(\mathbb{F}^n) := \lbrace L \in \mathcal{L}(\mathbb{F}^n, \mathbb{F}^n) \
\vert \ L \text{ is invertible} \rbrace
\end{align}
\ie, for every $L \in \aut(\mathbb{F}^n)$ there is a matrix $A \in
GL_n(\mathbb{F})$ such that $L = S_A$.
\item The mapping $\Sigma : \aut(\mathbb{F}^n) \rightarrow \aut(\mathbb{F}^n)$,
$\Sigma(L_A) := S_A$ is a one-to-one correspondence on the set of invertible
linear operators.
\end{enumerate}
\end{theorem}

\begin{proof}~\\
\begin{enumerate}
\item Due to the equality $A(\alpha S_A x + \beta S_A y) = \alpha A S_A x +
\beta A S_A y = \alpha x + \beta y$, $\alpha S_A x + \beta S_A y$ is the
unique solution of the system $Aw = \alpha x + \beta y$. Using
Definition~\ref{conclusion:def:solver-linop} it follows that $S_A(\alpha x +
\beta y) = \alpha S_A x + \beta S_A y$.
\item Since $L_A S_A x = A S_A x = x$ for $x \in \mathbb{F}^n$, $L_A$ is the
inverse for $S_A$.
\item If $S_A \neq S_B$ then there exists $x \in \mathbb{F}^n$ such that $S_A x
\neq S_B x$. Since $A$ is nonsingular, multiplying the inequality with $A$
results in $A S_B x \neq x$. This means that $A \neq B$, as otherwise $x = A S_A
x = A S_B \neq x$.
\item $S_A = S_B$ implies that for each column $a_j$ of $A$, $A S_B a_j = A S_A
a_j = a_j$. Since $e_j$ is the unique solution to $Ax = a_j$ this means that
$S_B a_j = e_j$. Multiplying the final equality by $B$ yields $b_j = B e_j = B
S_B a_j = a_j$, resulting in $A = B$.
\item Every $L \in \aut(\mathbb{F}^n)$ has an inverse linear operator $L^{-1}$.
For $L$ and $L^{-1}$ denote by $B$ and $A$ the matrices for which $L = L_B$ and
$L^{-1} = L_A$. Since $ABx = L_A L_B x = L^{-1}Lx = x$ for every
$x \in \mathbb{F}^n$, it follows that $AB = I$ and $\det(A)\det(B) = \det(AB) =
\det(I) = 1$. Thus, $\det(A) \neq 0$ and $A \in GL_n(\mathbb{F})$. Since $A(Lx)
= L_A(Lx) = L^{-1}(Lx) = x$ for $x \in \mathbb{F}^n$, $L$ is the solver operator
for $A$, \ie $L = S_A$.
\item The matrix $B$ from the previous point is also nonsingular since
$\det(A)\det(B) = 1$ implies $\det(B) \neq 0$. This means that the restriction
of the injective mapping $A \mapsto L_A$ to the set $GL_n(\mathbb{F})$ is also
surjective onto the set $\aut(\mathbb{F}^n)$. Thus, it establishes a one-to-one
correspondence between the sets $GL_n(\mathbb{F})$ and $\aut(\mathbb{F}^n)$.
The mapping $\Sigma$ is a composition of two bijective mappings $L_A
\mapsto A$ and $A \mapsto S_A$, so it is itself a one-to-one correspondence.
\end{enumerate}
\end{proof}

Theorem~\ref{conclusion:thm:solver-linop} shows that there is a correspondence
between a set of linear systems with a fixed nonsingular matrix (which can be
represented as a class of linear systems generated by that matrix) and
invertible operators.  In addition, since
Theorem~\ref{conclusion:thm:matrix-linop} established a link between matrices
and linear operators, the matrix can be replaced by its corresponding linear
operator, resulting in a well-defined permutation $\Sigma$ on linear operators,
which maps a linear operator $L$ to a linear operator that represents the
solution of a system generated by $L$.

\begin{definition}
\label{conclusion:def:solver-factory}
The higher-order permutation $\Sigma : \aut(\mathbb{F}^n) \rightarrow
\aut(\mathbb{F}^n)$ defined with $\Sigma(L_A) := S_A$, for $L_A \in
\aut(\mathbb{F}^n)$ is called the \textdef{solver factory} on
$\aut(\mathbb{F}^n)$.
\end{definition}

The final piece of the puzzle required to replace the concept of linear systems
with linear operators when designing the library is showing that the algebraic
structure of operations on linear systems is preserved when transforming them
into linear operators.

\begin{corollary}
\label{conclusion:cor:solver-linop}
Let $\Psi : (Ax = \cdot) \mapsto L_A$ be the mapping between linear systems wit
a fixed matrix $A$ and matrix product operators with respect to $A$ (this is a
bijection due to Theorem~\ref{conclusion:thm:matrix-linop}).
Then, the mapping $\Sigma \circ \Psi$ is an isomorphism between the set of
linear systems with a fixed matrix $A \in GL_n(\mathbb{F})$ and invertible
linear operators $\aut(\mathbb{F}^n)$ that preserves the algebraic structure of
the conjugate transpose operation and the operation of solving a linear system,
that is:
\begin{enumerate}
\item Solving a linear system with a fixed matrix $A$ is equivalent to the
application of the solver operator $S_A$: $Ax = b$ if and only if $x = S_Ab$,
for $x, b \in \mathbb{F}^n$.
\item Solving a linear system with the conjugate transpose of a matrix $A$ is
equivalent to the application of the solver operator's adjoint $S_A^*$:
$A^*x = b$ if and only if $x = S_A^*b$, for $x, b \in \mathbb{F}^n$.
\end{enumerate}
\end{corollary}

\begin{proof}
Claim 1 is true by construction of $S_A$. From
Theorem~\ref{conclusion:thm:solver-linop}, $S_A = (S_A^{-1})^{-1} = (L_A)^{-1} =
L_{A^{-1}}$, which implies $S_A^* = L_{A^{-1}}^* = L_{(A^{-1})^*} =
L_{(A^*)^{-1}} = L_{A^*}^{-1} = (S_{A^*}^{-1})^{-1} = S_{A^*}$. Claim 2 is then
obtained by substituting $A$ with $A^*$ in Claim 1 and using $S_{A^*} = S_A^*$.
\end{proof}

\subsection{Preconditioners}
TODO: Derivation of the preconditioner linear operator $P^\Pi_A$.

\subsection{Linear Operators --- Towards a Generic Interface for Sparse
            Computations}
TODO: The linear operator and linear operator factory abstractions as the basis
      of the library for sparse computations.
      If I feel like it, maybe I add the new ideas I have about having subspaces
      as first-class citizens in the library, which I feel are a better
      abstraction than vectors we're usually talking about - both for linear
      solvers, and for getting towards eigensolvers. Just to write it down
      somewhere, and maybe someone adds them to Ginkgo.
\subsection{Numerical Methods}
TODO: The issue of numerics and convergence - rounding errors affecting
matrices, preconditioners and solvers, limited convergence affecting solvers and
some preconditioners. The difference between the interface (what we want to
compute) and the numerical method (what we are actually computing), i.e. the
response to ``a Krylov solver is not a linear operator'' comment.

\subsection{Ginkgo: A High Performance Linear Operator Library}
TODO: The Ginkgo library implementing the things discussed above. Open and
modern development practices.

