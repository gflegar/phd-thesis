\section{Designing Scientific Software for Sparse Computations}

With the previous chapters of this work focusing on the components for the
solution of linear systems, the natural next consideration is the integration of
these components into a holistic software ecosystem for the solution of linear
systems.  A central challenge in this context is the manifoldness that arises
from combining different components. Each matrix format can be used with any Krylov
method. On top of that, each combination can be enhanced with any
preconditioner, which could be available directly as a matrix provided by the
user (stored in one of the formats supported by the software, or even in a
custom format), generated from the system matrix as one of the standard
preconditioners (\eg, relaxation or factorization based), or even implemented as
a coarse (Krylov) solver. The system matrix may not even be stored explicitly,
but available as a specialized implementation provided by the user \ha{but be
available implicitly via a user-defined vector norm} \gf{can you double
check this? vector norm? matrix-vector product cannot be replaced by a norm.
It's actually a user-defined linear operator, but we don't mention the
equivalence until later}. The preconditioners themselves may also be complex
methods for the solution of linear systems, as even the most common ILU-based
preconditioners can be constructed via selecting components from a pool of
factorization methods that generate a preconditioner, and a pool of linear
system solution methods that are used to solve systems with the upper and lower
triangular factors arising from the incomplete factorization. Another aspect
increasing the complexity of software design is the heterogeneity of modern
hardware. If the goal of the software is to support computation on various
devices, or even collaborative computation on a heterogeneous platform, its
design has to incorporate abstractions that allow reasoning about the underlying
hardware.

While there are various high performance libraries that try to address these
aspects~\cite{magma,vienna-cl,paralution}, they usually struggle with at least
one of them. This section proposes one possible theoretical design that aims at
solving these issues and briefly describes a new library that is based on it.

\subsection{Matrices}
\label{conclusion:ssec:matrices}

A central concept for reducing complexity in both mathematics and software
engineering is abstraction. By hiding all the unnecessary details of a
mathematical or software object, and only representing it as a concept with
certain operations that behave according to some rules, every concrete object
that defines these operations and follows the same rules can be
handled in the exact same way. In the context of Krylov methods, matrices are
used only as part of the matrix-vector product operation $y = Ax$. In rare
cases, the conjugate transpose matrix (\textdef{adjoint matrix}) $A^*$ is also
needed as part of the conjugate transpose matrix-vector product operation $y =
A^*x$. For a fixed matrix $A$, the key properties of the matrix-vector product
are \textdef{additivity} $A(x+y) = Ax + Ay$ and \textdef{homogeneity}
$A(\alpha x) = \alpha Ax$ --- which are often written combined into the
equivalent \textdef{linearity} property $A(\alpha x + \beta y) = \alpha Ax +
\beta Ay$.
\ha{where did you get homogeneity from? associativity
\url{https://en.wikipedia.org/wiki/Associative_property} and komul}
\gf{I've used the jargon from functions here, where additivity means $f(x + y) =
f(x) + f(y)$ and homogeneity (of degree 1) $f(\alpha x) = \alpha f(x)$. In this
case $f$ would be defined as $f : x \mapsto Ax$. This is something completely
natural if one knows about the isomorphism between linear operators and
matrices, but I see it is a bit strange to do it before we actually establish
it. Since I don't know how else to call it (the first one could be called
distributivity, but there is no one-word name for the second one), 
I removed the parenthesis  around the mathematical expression and put the terms
in italic, to make the statement define these terms as replacements for the
formulas written behind them.
additivity: \url{https://en.wikipedia.org/wiki/Additive_function};
homogeneity: \url{https://en.wikipedia.org/wiki/Homogeneous_function}}
Focusing only on these properties allows us to define an abstraction that can
hide the details about the coefficients and the storage scheme of the matrix.

\begin{theorem}
\label{conclusion:thm:matrix-linop}
For any matrix $A \in \mathbb{F}^{m \times n}$, let $L_A : \mathbb{F}^n
\rightarrow \mathbb{F}^m$ be an operator that satisfies $L_Ax = Ax$, $x \in
\mathbb{F}^n$. The following holds:
\begin{enumerate}
\item $L_A$ is a linear operator, \ie $L_A(\alpha x + \beta y) = \alpha L_A x +
\beta L_A y$, for any $\alpha,\beta \in \mathbb{F}$ and $x, y \in \mathbb{F}^n$.
\item $L_A$ is unique, \ie, if $L_A \neq L_B$ then $A \neq B$, for any $A, B
\in \mathbb{F}^{m \times n}$.
\item The mapping $A \mapsto L_A$ is injective, \ie, if $L_A = L_B$ then $A
= B$, for any $A, B \in \mathbb{F}^{m \times n}$.
\item The mapping $A \mapsto L_A$ is surjective onto the set of linear operators
\begin{align}
\mathcal{L}(\mathbb{F}^n,\mathbb{F}^m) := \lbrace L : \mathbb{F}^n \rightarrow
\mathbb{F}^m \ \vert \ L(\alpha x + \beta y) = \alpha Lx + \beta Ly, \ \forall
\alpha, \beta \in \mathbb{F}, \forall x, y \in \mathbb{F}^n \rbrace,
\end{align}
\ie, for every $L \in \mathcal{L}(\mathbb{F}^n,\mathbb{F}^m)$ there is a
matrix $A \in \mathbb{F}^{m \times n}$ such that $L = L_A$.
\end{enumerate}
Thus, $A \mapsto L_A$ defines a one-to-one correspondence between the set of
matrices $\mathbb{F}^{m \times n}$ and linear operators
$\mathcal{L}(\mathbb{F}^n, \mathbb{F}^m)$.
\end{theorem}
\begin{proof}~\\
\begin{enumerate}
\item This is a trivial consequence of the definition of $L_A$: $L_A(\alpha x +
\beta y) = A(\alpha x + \beta y) = \alpha Ax + \beta Ay = \alpha L_Ax + \beta
L_Ay$.
\item Assume $L_A \neq L_B$. Then, there exists a vector $x$ such that $Ax =
L_Ax \neq L_B x = Bx$ In consequence, $(A - B)x \neq 0$. This is only
possible if $A - B \neq 0$, \ie $A \neq B$.
\item For $i = 1,2, \ldots, n$, let $a_i$ and $b_i$ denote the $i$-th column of
matrices $A$ and $B$, respectively, and let $e_i \in \mathbb{F}^n$ be the $i$-th
vector of the standard basis for $\mathbb{F}^n$, \ie, the $i$-th component of
$e_i$ is $1$, while its every other component is $0$. Then, $a_i = Ae_i = L_Ae_i
= L_B e_i = Be_i = b_i$, for every column $i$, which implies $A = B$.
\item For any linear operator $L \in \mathcal{L}(\mathbb{F}^n, \mathbb{F}^m)$ we
construct the matrix $A \in \mathbb{F}^{m \times n}$ by defining its $i$-th
column as $a_i := Le_i$. Then, for any vector $x = [ x_1, x_2, \ldots, x_n ]^T 
\in \mathbb{F}^n$, the following holds:
\begin{align}
L_Ax &= Ax = A(\sum_{i=1}^n x_i e_i) j = \sum_{i=1}^n x_i Ae_i \nonumber \\
     &= \sum_{i=1}^n x_i a_i = \sum_{i=1}^n x_i Le_i
      = L(\sum_{i=1}^n x_i e_i) \label{conclusion:eqn:surjection} \\
     &= Lx, \nonumber
\end{align}
generating the equality $L_A = L$.
\end{enumerate}
\end{proof}

\begin{definition}
For any matrix $A \in \mathbb{F}^{m \times n}$, the unique linear operator $L_A
\in \mathcal{L}(\mathbb{F}^n, \mathbb{F}^m)$ defined by $L_Ax := Ax$ for $x \in
\mathbb{F}^n$ is called the \textdef{matrix product operator} with respect
to $A$.
\end{definition}

Theorem~\ref{conclusion:thm:matrix-linop} shows that every matrix can be
uniquely represented by its matrix product operator, and that a specific matrix
can be reconstructed given its linear operator representation. However, to
effectively replace matrices with linear operators, the structure of algebraic
operations defined on matrices has to be mirrored by linear operators.

\begin{definition}
Let $L \in \mathcal{L}(\mathbb{F}^n, \mathbb{F}^m)$ be a linear operator and
let $(\cdot, \cdot)_m : \mathbb{F}^m \times \mathbb{F}^m \rightarrow
\mathbb{F}$ and $(\cdot, \cdot)_n : \mathbb{F}^n \times \mathbb{F}^n
\rightarrow \mathbb{F}$ be any two inner products~\cite{la} on vector spaces
$\mathbb{F}^m$ and $\mathbb{F}^n$, respectively. A linear operator $K \in
\mathcal{L}(\mathbb{F}^m, \mathbb{F}^n)$ that satisfies
\begin{align}
    (Lx, y)_m = (x, Ky)_n \label{conclusion:eqn:adjoint}
\end{align}
for every $x \in \mathbb{F}^n$ and $y \in \mathbb{F}^m$ is called an
\textdef{adjoint operator} of $L$ with respect to inner products $(\cdot,
\cdot)_m$ and $(\cdot, \cdot)_n$, and is denoted by $L^*$.
\end{definition}

\begin{theorem}
\label{conclusion:thm:adjoint}
For any matrix $A \in \mathbb{F}^{m \times n}$, the adjoint operator $L_A^*$ of
the matrix product operator $L_A$ with respect to inner products $(x, y)_m :=
x^*y, \ \forall x,y \in \mathbb{F}^m$ and $(w, z)_n := w^*z, \ \forall w, z \in
\mathbb{F}^n$ is unique, and $L_A^* = L_{A^*}^{}$.
\end{theorem}

\begin{proof}
Let $K$ be any adjoint operator for $L_A$. Then, applying
Equation~(\ref{conclusion:eqn:adjoint}) on $e_i \in \mathbb{F}^n$ and $e_j \in
\mathbb{F}^m$ yields:
\begin{align}
    (Ke_j)_i &= e_i^*Ke_j^{} = (e_i, Ke_j)_n = (L_Ae_i, e_j)_m \nonumber \\
             &= (Ae_i,  e_j)_m = (a_i, e_j)_m =a_i^*e_j^{} = \overline{a_{ji}}
             \\
             &= (A^*)_{ij} \nonumber
\end{align}
The equality $K = L_{A^*}$ is obtained by applying
Equation~(\ref{conclusion:eqn:surjection}) to $L_{A^*}$ and $K$.
\end{proof}

\begin{corollary}
\label{conclusion:cor:matrix-linop}
The mapping $A \mapsto L_A$ is an isomorphism between the set of
matrices $F^{m \times n}$ and linear operators $\mathcal{L}(\mathbb{F}^n,
\mathbb{F}^m)$ that preserves the algebraic structure of the matrix-vector
multiplication and conjugate transpose operations, that is:
\begin{enumerate}
\item Matrix-vector multiplication is equivalent to the application of the
matrix product operator: $Ax = L_Ax$, for $x \in \mathbb{F}^n$.
\item Conjugate transpose matrix-vector multiplication is equivalent to the
application of the matrix product operator's adjoint: $A^*x = L_A^*x$, for $x
\in \mathbb{F}^n$.
\end{enumerate}
\end{corollary}

\begin{proof}
Relation 1 is true by the definition of operator $L_A$. Relation 2 is a direct
consequence of Relation 1 and Theorem~\ref{conclusion:thm:adjoint}.
\end{proof}

\subsection{Linear Systems}
\label{conclusion:ssec:systems}

Another central operation that a library focused on Krylov solvers has to
provide is, obviously, the solution of a linear system. Interestingly, this
operation satisfies the same additivity and homogeneity properties as the
matrix-vector multiplication.

\begin{definition}
\label{conclusion:def:solver-linop}
For any nonsingular matrix $A \in GL_n(\mathbb{F}) := \lbrace M \in \mathbb{F}^n
\ \vert \ \det(M) \neq 0 \rbrace$, let $S_A : \mathbb{F}^n \rightarrow
\mathbb{F}^n$ be an operator that satisfies $A(S_Ab) = b$, for $b \in
\mathbb{F}^n$. That is, the operator $S_A$ maps any (right-hand side) vector $b$
to the solution $x$ of the linear system $Ax = b$ induced by the system matrix
$A$ and the right-hand side $b$. Such an operator is called a \textdef{solver
operator} with respect to $A$.
\end{definition}

\begin{theorem}
\label{conclusion:thm:solver-linop}
For any $A \in GL_n(\mathbb{F})$:
\begin{enumerate}
\item $S_A$ is a linear operator, \ie $S_A(\alpha x + \beta y) = \alpha S_A x +
\beta S_A y$, for any $\alpha,\beta \in \mathbb{F}$ and $x, y \in \mathbb{F}^n$.
\item $S_A$ is invertible and $S_A^{-1} = L_A^{}$.
\item $S_A$ is unique, \ie, if $S_A \neq S_B$ then $A \neq B$, for any $A, B
\in GL_n(\mathbb{F})$.
\item The mapping $A \mapsto S_A$ is injective, \ie, if $S_A = S_B$ then $A
= B$, for any $A, B \in GL_n(\mathbb{F})$.
\item The mapping $A \mapsto S_A$ is surjective onto the set
\begin{align}
\aut(\mathbb{F}^n) := \lbrace L \in \mathcal{L}(\mathbb{F}^n, \mathbb{F}^n) \
\vert \ L \text{ is invertible} \rbrace
\end{align}
\ie, for every $L \in \aut(\mathbb{F}^n)$ there is a matrix $A \in
GL_n(\mathbb{F})$ such that $L = S_A$.
\item The mapping $\Sigma : \aut(\mathbb{F}^n) \rightarrow \aut(\mathbb{F}^n)$,
$\Sigma(L_A) := S_A$ is a one-to-one correspondence on the set of invertible
linear operators.
\end{enumerate}
\end{theorem}

\begin{proof}~\\
\begin{enumerate}
\item Due to the equality $A(\alpha S_A x + \beta S_A y) = \alpha A S_A x +
\beta A S_A y = \alpha x + \beta y$, $\alpha S_A x + \beta S_A y$ is the
unique solution of the system $Aw = \alpha x + \beta y$. From
Definition~\ref{conclusion:def:solver-linop} it follows that $S_A(\alpha x +
\beta y) = \alpha S_A x + \beta S_A y$.
\item Since $L_A S_A x = A S_A x = x$ for $x \in \mathbb{F}^n$, $L_A$ is the
inverse for $S_A$.
\item If $S_A \neq S_B$ then there exists $x \in \mathbb{F}^n$ such that $S_A x
\neq S_B x$. Since $A$ is nonsingular, multiplying the inequality with $A$
results in $A S_B x \neq x$. This means that $A \neq B$, as otherwise $x = A S_A
x = A S_B \neq x$.
\item $S_A = S_B$ implies that for each column $a_j$ of $A$, $A S_B a_j = A S_A
a_j = a_j$. Since $e_j$ is the unique solution of $Ax = a_j$ this means that
$S_B a_j = e_j$. Multiplying the final equality by $B$ yields $b_j = B e_j = B
S_B a_j = a_j$, resulting in $A = B$.
\item Every $L \in \aut(\mathbb{F}^n)$ has an inverse linear operator $L^{-1}$.
For $L$ and $L^{-1}$ denote by $B$ and $A$ the matrices for which $L = L_B$ and
$L^{-1} = L_A$. Since $ABx = L_A L_B x = L^{-1}Lx = x$ for every
$x \in \mathbb{F}^n$, it follows that $AB = I$ and $\det(A)\det(B) = \det(AB) =
\det(I) = 1$. Thus, $\det(A) \neq 0$ and $A \in GL_n(\mathbb{F})$. Since $A(Lx)
= L_A(Lx) = L^{-1}(Lx) = x$ for $x \in \mathbb{F}^n$, $L$ is the solver operator
for $A$, \ie $L = S_A$.
\item The matrix $B$ in (5) is also nonsingular since $\det(A)\det(B) = 1$
implies $\det(B) \neq 0$. This means that the restriction of the injective
mapping $A \mapsto L_A$ to the set $GL_n(\mathbb{F})$ is also surjective onto
the set $\aut(\mathbb{F}^n)$. Thus, it establishes a one-to-one correspondence
between the sets $GL_n(\mathbb{F})$ and $\aut(\mathbb{F}^n)$.  The mapping
$\Sigma$ is a composition of two bijective mappings $L_A \mapsto A$ and $A
\mapsto S_A$, so it is itself a one-to-one correspondence.
\end{enumerate}
\end{proof}

Theorem~\ref{conclusion:thm:solver-linop} shows that there is a correspondence
between a set of linear systems with a fixed nonsingular matrix (which can be
represented as a class of linear systems generated by that matrix) and
invertible operators.  In addition, since
Theorem~\ref{conclusion:thm:matrix-linop} established a link between matrices
and linear operators, the matrix can be replaced by its corresponding linear
operator, resulting in a well-defined permutation $\Sigma$ on linear operators,
which maps a linear operator $L$ to a linear operator that represents the
solution of a system generated by $L$.

\begin{definition}
\label{conclusion:def:solver-factory}
In the context of linear algebra software design, we call the higher-order
permutation $\Sigma : \aut(\mathbb{F}^n) \rightarrow \aut(\mathbb{F}^n)$ defined
with $\Sigma(L_A) := S_A$, for $L_A \in \aut(\mathbb{F}^n)$ the \textdef{solver
factory} on $\aut(\mathbb{F}^n)$.
\end{definition}

The final piece of the puzzle required to replace the concept of linear systems
with linear operators when designing the library is showing that the algebraic
structure of operations on linear systems is preserved when transforming them
into linear operators.

\begin{corollary}
\label{conclusion:cor:solver-linop}
Let $\Psi : (Ax = \cdot) \mapsto L_A$ be the mapping between linear systems wit
a fixed matrix $A$ and matrix product operators with respect to $A$ (this is a
bijection due to Theorem~\ref{conclusion:thm:matrix-linop}).
Then, the mapping $\Sigma \circ \Psi$ is an isomorphism between the set of
linear systems with a fixed matrix $A \in GL_n(\mathbb{F})$ and invertible
linear operators $\aut(\mathbb{F}^n)$ that preserves the algebraic structure of
the conjugate transpose operation and the operation of solving a linear system,
that is:
\begin{enumerate}
\item Solving a linear system with a fixed matrix $A$ is equivalent to the
application of the solver operator $S_A$, \ie, $Ax = b$ if and only if $x =
S_Ab$, for $x, b \in \mathbb{F}^n$.
\item Solving a linear system with the conjugate transpose of a matrix $A$ is
equivalent to the application of the solver operator's adjoint $S_A^*$, \ie,
$A^*x = b$ if and only if $x = S_A^*b$, for $x, b \in \mathbb{F}^n$.
\end{enumerate}
\end{corollary}

\begin{proof}
(1) follows from the construction of $S_A$. From
Theorem~\ref{conclusion:thm:solver-linop}, $S_A = (S_A^{-1})^{-1} = (L_A)^{-1} =
L_{A^{-1}}$, which implies $S_A^* = L_{A^{-1}}^* = L_{(A^{-1})^*} =
L_{(A^*)^{-1}} = L_{A^*}^{-1} = (S_{A^*}^{-1})^{-1} = S_{A^*}$. (2) is then
obtained by substituting $A$ with $A^*$ in (1) and using $S_{A^*} = S_A^*$.
\end{proof}

\subsection{Preconditioners}

Preconditioning (replacing the original linear system $Ax = b$ with a
preconditioned system, \eg $M^{-1}Ax = M^{-1}b$ for right preconditioning, where
$M$ is some approximation of $A$) is another central concept in the iterative
solution of linear systems. While the preconditioner $M$ is often
represented by a matrix, the only operations where the preconditioner appears
are its application to a vector $y = M^{-1}x$, and sometimes the application of
its conjugate transpose to a vector $y = (M^{-1})^*x$. The preconditioner matrix
$M$ is associated to the original system matrix $A$ via the method $\Pi$ that
was used to derive the preconditioner from the system matrix $A$. A
preconditoner needs to be nonsingular to preserve the existence and uniqueness
of the linear system's solution. These considerations motivate the following
definition:

\begin{definition}
\label{conclusion:def:precond-linop}
Any mapping $\Pi : \aut(\mathbb{F}^n) \rightarrow \aut(\mathbb{F}^n)$ is called
a \textdef{preconditioner factory}. For any operator $L_A \in
\aut(\mathbb{F}^n)$ (or equivalently, matrix $A \in GL_n(\mathbb{F})$) the
linear operator $P_A^\Pi := \Pi(L_A)$ is called the \textdef{preconditioner
operator} with respect to the operator $L_A$ (or equivalently, $A$) and the
preconditioner factory $\Pi$.  The unique matrix $M \in GL_n(\mathbb{F})$ such
that $P_A^\Pi = L_{M^{-1}}$ is called the \textdef{preconditioner matrix} with
respect to $A$ and $\Pi$.
\end{definition}

An isomorphism $M \mapsto P_A^\Pi$ between precondtioner matrices and
precondtioner operators that preserves the algebraic structure of preconditoner
application and conjugate transpose preconditioner application is established
via Corollary~\ref{conclusion:cor:matrix-linop}. Thus, precondtioners easily
join the family of linear algebra operations that can be represented as linear
operators.

\ha{We note that preconditioners are not always available in explicit form. In
    some cases, they are only implicitly available as (a few) iterations of an
    iterative solver (Jacobi/Gauss-Seidel), in some cases they are available in
    factorized form (ILU).}
\ha{Thus, even though not always available in explicit form, well-defined
preconditioners can be represented as linear operators.}
\gf{I feel this is off-topic here. This section talks about the interface, and
how a preconditioner (which is usually represented by a matrix $M$, at least in
all the literature about Krylov solvers) can be expressed as a linear operator,
so the interface can stick with linear operators, instead of making up a new
concept for preconditioners. Whether or not that matrix $M$ is represented
explicitly (whatever your definition of ``explicitly'' means: is a row-major
dense matrix stored explicitly? a sparse matrix in CSR? a sparse matrix stored
as a sum in the context of the hybrid format? a matrix stored as two factors?
how about a hierarchical matrix with factorized blocks?) are implementation
details that are (in good software design) completely orthogonal to the issues
of the interface.
Another point for this and all previous sections is that everything is
discussed from the purely mathematical point of view. Just the fact that I
mention $A^{-1}$ does not imply that I'm actually trying to compute $A^{-1}$.
Just because I mention the preconditioner matrix $M$ and its invers $M^{-1}$
which is being applied to a vector, does not mean that I'm actually trying to
compute them (again - whatever your definition of ``compute'' may be). This is
further elaborated in the following section, which points out the fact that the
actual software does not exactly mirror everything that is discussed in the
mathematical background, but works around the restrictions imposed by the real
world (computability, finite arithmetic) by using other methods of approximating
the effect of the math-backed interface.
}

\subsection{Linear Operators --- Towards a Generic Interface for Sparse
            Computations}
The previous sections elaborated on how to represent the major components of an
iterative solution framework --- matrices and matrix-vector product,
preconditioners and preconditioner application, and linear systems and their
solution --- as linear operators. Using this abstraction in the design of a
library effectively solves the problem of various component combinations for the
solution of linear systems.  Every component is implemented in terms of the
linear operator abstraction (\eg a Krylov method uses the abstract interface of
the linear operator to realize matrix-vector products and preconditioner
applications), and the decision of which concrete linear operators should be
used for the components is left to the top-level application. The ability to
substitute an abstract linear operator by a concrete implementation allows
users to express any of the complex component combinations presented at the
beginning of this chapter. Another advantage is that support for
application-specific matrix formats, precondtioners, or methods for the solution
of linear systems can be enabled by simply leaving the concept of the linear
operator open for extension. This allows users to provide their own
implementations of components that interoperate with the rest of the library in
the same way the built-in components do.

While the linear operator abstraction is perfectly suited for software that
computes linear algebra operations symbolically, numerical libraries need to
adopt a small approximation for these abstractions to work. The usual
argument against the linear operator abstraction is the fact that a fixed number
of iterations of a Krylov method does not represent a linear operator. Indeed,
for any Krylov method, the linear combination $\alpha x_k + \beta y_k$ of $k$-th
approximations $x_k$ and $y_k$ for the systems $Ax = b$ and $Ay = c$ is
generally not equal to the $k$-th approximation $z_k$ for the system $Az =
\alpha b + \beta c$. This is not the only ``flaw'' that can be found in a
concrete implementation of the linear operator abstraction.  In fact, in a
numerical setting with limited precision, none of the linear operators can be
realized accurately.
A natural implementation of the matrix product operator is
a dense or sparse matrix-vector
multiplication, using floating point arithmetic. Thus, these computations will
contain rounding errors and the computed outputs $\widetilde{x}$,
$\widetilde{y}$ and $\widetilde{z}$ for operations $x = Ab$, $y = Ac$ and $z =
A(\alpha b + \beta c)$, respectively, will not satisfy $\widetilde{z} = \alpha
\widetilde{x} + \beta \widetilde{y}$.  The same aspects arise for the
preconditioner application. In consequence, the only way to obtain accurate
matrix product and preconditioner operators is to use infinite precision
arithmetic, which is usually to expensive and not necessary. In the case of
Krylov methods, that are used to compute the application of the solver operator
$S_A$, the accurate solution that satisfies the additivity and homogeneity
properties could be obtained by using infinite precision arithmetic and running
the method until full convergence is reached.

The short answer to the observation about Krylov methods is that the method is
not intended to be used as a linear operator $S_A$, but as a way of
approximating the effect of applying the linear operator $S_A$ to a vector.
In more detail, the main distinction between the solver operator (which is a
linear operator) and any of the Krylov methods (which are strictly speaking not
linear operators) is that the former is used to described what is being
computed, and the latter how is it being computed.
Assuming there is an implicit consensus that every operation in the software
library comes with some approximation error --- which is a common concept in
computational numerical linear algebra --- the linear operator abstraction
continues to be valid.

\begin{definition}
\label{conclusion:def:apprximate-linop}
Let $L \in \mathcal{L}(\mathbb{F}^n,\mathbb{F}^m)$ be any linear operator and $E
: \mathbb{F}^n \rightarrow \mathbb{F}^m$ be any (non-linear) function.
The function $\widetilde{L} : \mathbb{F}^n \rightarrow \mathbb{F}^m$,
$\widetilde{L}(x) := Lx + E(x)$ is called an \textdef{approximate linear
operator} for $L$ with approximation error $E$.
\end{definition}

Strictly speaking, the approximate linear operator does not satisfy the linearity
property. However, it does satisfy a relaxed version of linearity:
$\widetilde{L}(\alpha x + \beta y) = \alpha \widetilde{L}(x) + \beta
\widetilde{L}(y) + (E(\alpha x + \beta y) - \alpha E(x) - \beta E(y))$. Thus,
the smaller the approximation error, the closer is the approximate linear
operator to fulfilling the defining characteristics of a linear operator. From
the numerical linear algebra software design perspective, all concrete
implementations of linear operators are approximate linear operators. However,
this fact is only revealed during the construction of the approximate linear
operator, where the user can sometimes influence the magnitude of the
approximation error $E$. Once the approximate linear operator is constructed, it
is treated as a linear operator, since exposing the unnecessary details about
the approximation error would only complicate the design without offering any
practical advantages.


\subsection{Ginkgo --- A High Performance Linear Operator Library}

An example of a software ecosystem building upon the concept of linear operators
is the Ginkgo open source library~\cite{ginkgo}. The library was developed to
overcome the problems with composability, heterogeneous hardware support and
extensibility in existing solutions. In addition to linear operators, Ginkgo
adds orthogonal abstractions representing hardware implementation aspects.
Precisely, Ginkgo employs ``executors'' that link the data with the appropriate
memory spaces and the operations with computational resources. It implements
several matrix formats, which represent matrix product operators, Krylov
methods, which approximate the appliction of the solver operator, as well as
preconditioners, which enhance the convergence rate of the Krylov methods. The
linear operator abstraction is left open to extension to allow user-defined
implementations of matrix formats, Krylov methods, and precondtioners. To
simplify the adaptation of existing methods to specific needs, several other
extensible abstractions are provided. These include the ability to modify the
stopping criteria used to check convergence of iterative methods, as well as
logging hooks, which enable to interactively track the progress of the distinct
operations.

\begin{figure}
\begin{center}
\lstinputlisting[language=C++]{code/solver.cpp}
\end{center}
\caption
[A simple example of using Ginkgo to solve a linear system]
{A simple example of using Ginkgo to solve a linear system.}
\label{conclusion:fig:ginkgo-example}
\end{figure}

Figure~\ref{conclusion:fig:ginkgo-example} shows an example where Ginkgo is used
to iteratively solve a linear system. Line~5 defines the CUDA device with id 0
as the execution space where the operations will be performed. The linear system
data, including the input matrix, the right hand side, and the initial guess is
read from standard input and stored in linear operators \texttt{A}, \texttt{b}
and \texttt{x} on Lines~6--8. Then, Lines~9--17 define a specific instantiation
of the solver factory $\Sigma$ (denoted by the \texttt{solver} variable in the
code) which will be used to generate the solver operator. The solver operators
generated by this specific instantiation of the factory will use the conjugate
gradient (CG) Krylov method to approximate its application (Line~10). The method
is executed for at most 1000 iterations (Line~13), or until the original
residual norm is reduced by 15 orders of magnitude (Lines~14--16), whichever
happens sooner. The Krylov method will be enhanced by a precondtioner generated
from the system matrix using the block-Jacobi preconditioner factory $\Pi$
(Line~11). This part of the example shows that, while there is a unique solver
factory $\Sigma$, libraries may allow multiple ``instantiations'' of it to
incorporate the fact that the same mathematical operation may be computed in
multiple ways, depending on the desired accuracy and problem characteristics.
The complete computation is done on Line~18, which generates the solver operator
and solves the system. The \texttt{generate} method uses the matrix product
operator \texttt{A} to create the corresponding solver operator.
Since the specified Krylov method is enhanced with preconditioners generated
by the block-Jacobi preconditioner factory, this factory's \texttt{generate}
method will be used to generate a block-Jacobi precondtiioner operator from the
matrix product operator \texttt{A}. Then, once the \texttt{apply} method is
called, the CG Krylov solver is employed to approximate the result of the solver
operator application, using the matrix product operator \texttt{A} and the
previously generated block-Jacobi preconditioner operator for \texttt{A}.
Finally, the approximate solution is written to standard output on Line~19.

\ha{
Maybe mention that also the composition of the linear solver and the
preconditioner is a linear operator? Maybe mention that earlier?}
\gf{
Difficult... First of all, what is a ``linear solver''? This is not something
defined anywhere in the text. If you're thinking about a Krylov solver, or even
a relaxation solver with a variable amount of iterations (\ie if the stopping
criterion is based on the residual), these do not satisfy the strict linearity,
even as mathematical concepts, so they themselves are not linear operators and
their composition with anything is also generally not a linear operator. If
you're talking about the solver operator defined previously, which is a linear
operator, then its composition (in the mathematical sense of function
composition) with any other linear operator (including any of the preconditioner
operators) is trivially also a linear operator. However, a mathematical
composition of these things is not something one is normally interested in, as
it results in $y = P_A^\Pi S_Ab$, which satisfies $AMy = b$, and not $Ay = b$.
Strictly speaking, preconditioners are used to enhance the method (not the
mathematical concept of the solver operator!) employed to approximate the
application of the solver operator, which belongs to the approximate linear
operator amendment described above. This is also the reason why I didn't use
your suggestion for the part of the text that describes the role of Krylov
solvers, as I feel that it somehow obfuscates this distinction.  Not sure
whether this is something that should be a discussed in more detail here in a
form of a remark, or maybe put in the FAQ section of Ginkgo if a lot of people
has the same question.
}
\ha{
Maybe mention the data is transferred "behind the scenes", i.e. the user does
not have to explicitly copy? Execution space and memory space are coupled...
}
\gf{
I think it's pretty clear by looking at the code that there are no explicit
copies. As for the coupling of execution and memory spaces, this is something
that will have to be significantly refined in a future version of Ginkgo, so I
don't want to go into to much details here, to avoid this thesis becoming
outdated to soon. In any case, I would rather focus here on motivating ginkgo
using its mathematical background, and leave an exhaustive description of all
its features for the interface paper or a whitepaper.
}
