\section{Conclusions and Open Research Directions}

This work focused on GPU acceleration of components for the iterative solution
of linear systems, and showed that significant performance improvements can be
obtained on modern hardware even with basic, well-studied building blocks.

\noindent \textbf{Sparse matrix-vector product.}
Part~\ref{pt:matrix-vector} started with a discussion on the most time
consuming and difficult to parallelize operation: the sparse matrix-vector
product. Even though a variety of advanced sparse matrix formats and
accompanying matrix-vector product algorithms were recently proposed,
Chapter~\ref{ch:2017-csr-spmv} demonstrated that most of them exhibit corner
cases where the formats either consume significantly more memory than the
standard formats, or achieve far lower matrix-vector product performance than
standard implementations. Furthermore, since the majority of existing software
packages provides high performance implementations of other operations, they are
often restricted to one of the standard formats as a means of managing software
complexity and developer burden. Thus, while application-specific formats can
undoubtedly outperform the conventional alternatives, they should only be
developed in case the potential improvements over the formats provided by the
underlying general-purpose library (accounting for the necessary format
conversions required for interfacing with other parts of the library) outweigh
their development cost.

Chapter~\ref{ch:2017-csr-spmv} focused on reducing the improvement potential of
specialized formats --- and the need to invest resources in the development of
application-specific formats --- by optimizing relevant corner cases of the most
widely used CSR format.  Enabled by advancements in accelerator technology,
which recently started offering full support for atomic operations, the proposed
optimizations effectively deal with the issue of imbalanced sparsity patterns.
While the classical approach still offers superior performance for regular
sparsity patterns, the ultimate matrix-vector product algorithm can be obtained
by coupling the two realizations with a simple heuristic that predicts the
winner, and selects the superior approach automatically.

Chapter~\ref{ch:2017-coo-spmv} continued the development of the sparse
matrix-vector product by identifying the COO format as an alternative
general-purpose format for GPUs. Similarly to the improved CSR algorithm, the
new kernel developed for the COO format is highly efficient and does not suffer
from extremely unfavorable sparsity patterns. Furthermore, its higher minimum
and average performance make it a better default choice than CSR. Ultimately,
reasonable use-cases can be found for both options: the improved CSR algorithm
can be used in conjunction with software that relies on CSR historically or in
case the efficiency of other operations depends on it. In contrast, COO can be
adopted as the default choice for new software whose performance does not depend
on CSR.

While most sparse matrix-vector product algorithms are focused on large
problems that utilize the entire GPU (processor group),
Chapter~\ref{ch:2017-batched-spmv} explored the underdeveloped case of smaller
problems suitable for individual streaming multiprocessors (single processors).
It demonstrated that this case can be implemented more efficiently by slightly
modifying standard algorithms to make better use of the available memory
hierarchy.

New findings presented in this part  can be used in the development of more
specialized matrix formats. For example, the ideas from the COO algorithm are
currently being used for the development of an improved hybrid matrix
format~\cite{hybrid}. The success of the synchronization-free load-balancing
algorithms on a single GPU should also prove to be useful for the development of
sparse matrix-vector product algorithms that utilize the computational resources
of the entire node, or even multiple nodes in unison as synchronization and
load-balancing penalties become more pronounced on higher levels of the
\textdef{hardware hierarchy} (vector unit, processor, processor group, node,
cluster). While this task is far from straightforward when targeting a general
matrix-vector product due to the unpredictable memory access and lack of atomic
operations on a distributed memory architecture, previous research on the topic
suggests that it may be possible to design effective algorithms if
problem-specific details are exploited in algorithm
design~\cite{distributed-spmv}. Finally, the
simple heuristic used to select between the two CSR algorithms is also a small
contribution to research in automatic sparse matrix format selection. This
research area focuses on selecting the best format based on the properties of
the matrix, by using certain policies (often based on machine learning
algorithms) to decide among several available
formats~\cite{clspmv,gpu-selection}, or even assemble a matrix-vector product
implementation from a pool of potential optimizations~\cite{elafrou}.

\noindent\textbf{Preconditioning.}
Part~\ref{pt:preconditioning} explored the potential of block-Jacobi
preconditioning on highly parallel GPU hardware. Even though this relatively
simple preconditioner usually features lower convergence improvement than
the popular ILU-based preconditioners, this part of the thesis showed that
problems with inherent block structure can greatly benefit from block-Jacobi
preconditioning.  The parallel performance of block-Jacobi can be attributed to
its inherent parallelism, as each block can be processed independently. The
first step towards a high performance implementation consists of mapping the
blocks to the appropriate level of the hardware hierarchy. By assigning each
block to a single vector unit, taking advantage of increased register counts and
warp shuffle instructions available on recent hardware, and replacing
conventional pivoting strategies with implicit pivoting, the resulting
algorithms are able to outperform equivalent functionality available in vendor
libraries. The first such algorithm, the block-Jacobi preconditioner based on
Gauss-Jordan elimination, was presented in
Chapter~\ref{ch:2017-gje-block-jacobi}. The algorithm inverts the diagonal
blocks during preconditioner generation, which means that the application stage
can be realized as a sequence of highly parallel dense used matrix-vector
products.

However, as discussed in Section~\ref{introduction:sec:linear-systems}, the
solution of a linear system via matrix inversion can result in numerical
instability. Chapter~\ref{ch:2017-gh-block-jacobi} addressed these concerns by
showing that, in practice, there is no difference in preconditioner quality
when using the explicit inversion-based scheme, as opposed to a
factorization-based approach. It also revisited the unconventional Gauss-Huard
method for the solution of linear systems, and revealed that this method can be
superior to the inversion-based algorithm if only few iterations of the Krylov
solver are needed. Finally, Chapter~\ref{ch:2017-lu-block-jacobi} compared the
Gauss-Huard solver with the standard LU factorization algorithm, and showed
that, provided the conventional ``lazy'' triangular solve algorithm is replaced
with the ``eager'' variant, the LU factorization can outperform the alternate
Gauss-Huard decomposition.

The contributions presented in this part constitute only a small sample of
recent developments in preconditioning techniques. Some of these developments
include new highly parallel methods for solving triangular
systems~\cite{triangular-solve, isai, triangular-iterative}, and parallel
generation of threshold ILU preconditioners~\cite{ilut, ilut-gpu}. As a direct
extension of the block-Jacobi preconditioner presented here, future research can
explore the effectiveness of other preconditioners based on relaxation methods.
An orthogonal research direction would include scaling up the block-Jacobi
preconditioner to distributed memory systems. Unlike the sparse matrix-vector
product, the regular structure of the block-Jacobi preconditioner allows for
relatively straightforward distributed algorithm, as each block can be moved to
the memory space that holds the corresponding segments of the input and output
vectors, and processed on the processor with direct access to that memory space.

All algorithms presented in this part are also related to a broader topic of
``batched'' routines, which apply the same operation on a sequence of small
problems. While there is a recent proposal for a standardized batched BLAS
interface~\cite{batched-blas}, it is still unclear whether this effort will result in
wide adoption, as there are major issues with the proposed interface.
One such issue concerns the data format used to store the blocks. Since distinct
architectures and applications require specific storage schemes (\eg, one
parameter batch is shared, another stored as a contiguous sequence, or scattered
throughout the memory), covering all the options leads to an exponential
growth of interface functions with an unmanageable number of parameters.
Another issue arises from the implicit synchronization between two consecutive
batched routine calls. While there are no dependencies between the distinct
problems forming a batch, the entire batch is still synchronized, as each
batched call is essentially a (parallel) loop over all problem instances. While
this can be partially alleviated by implementing the routines in terms of a set
of jobs submitted to a job scheduling system, relying on the existence of such a
system is not always an option. For example, this is the case for block-Jacobi
preconditioning presented in this work, where the ``batch'' of problems is
distributed on the GPU (where scheduling systems are not commonly used), and
there is additional code needed for preprocessing and postprocessing of the
problem data as part of the same GPU kernel. Ultimately, the pragmatic solution
might be to depart from the idea of ``batched'' routines, and instead build
libraries that provide BLAS-like functionality for various levels of the
hardware hierarchy.  Essentially, the responsibility of building the inherently
parallel outer loop would be left to the user, removing the need for complicated
parameter lists, greatly reducing the number of interface variations, and
avoiding implicit synchronizations of unrelated problems. While such libraries
are still uncommon, NVIDIA has recently taken a step in this direction with its
CUTLASS~\cite{cutlass} library, which provides matrix-matrix multiply
implementations (GEMMs) for various levels of the GPU hardware hierarchy.

\noindent\textbf{Adaptive precision.}
As a result of recent trends in HPC and the emergence of hardware support for
low precision arithmetic, Part~\ref{pt:adaptive} evaluated the potential of
employing low precision techniques in combination with preconditioning.
Chapter~\ref{ch:2017-adaptive-block-jacobi} analyzed the theoretical aspects and
the effect on convergence rate when lower precision storage is used in the
block-Jacobi preconditioner. The success of the approach is based on
several observations: 1) since the preconditioner application is bounded by the
memory bandwidth, performance improvements are possible by reducing the
precision of data stored in memory, and not the precision of computations; 2) a
preconditioner is already an approximation of the original system matrix, so the
storage accuracy does not need to be higher than the accuracy of the
approximation; and 3) storage accuracy cannot be reduced blindly, but has to be
carefully tuned to the numerical properties of the problem. Practical
experiments showed that the combination of these techniques successfully reduces
total storage while preserving preconditioner quality, and, according to a
theoretical energy model, can reduce energy consumption of the complete Krylov
solver. While a theoretical analysis alone does not guarantee the feasibility of
practical implementations, new research has demonstrated the effectiveness of
adaptive block-Jacobi preconditioning in practice~\cite{adaptive-jacobi-gpu},
and produced a first GPU implementation of this preconditioner, which is now
available in the Ginkgo library~\cite{ginkgo}.

Since the adaptive precision block-Jacobi represents a pioneering idea in
preconditioner design, it may be possible to enhance other preconditioners using
similar techniques. Adaptive block-Jacobi preconditioning itself may also be
improved further by using non-conventional storage formats instead of the
standard IEEE floating point types~\cite{adaptive-jacobi-gpu}. Exploiting
support for low-precision computing hardware, such as the tensor cores available
on the latest generations of NVIDIA GPUs is another possible research direction.
However, one would first have to escape the memory-bandwidth-bound nature of
preconditioner application (most likely through the use of block Krylov methods,
or simply by solving a problem with multiple right-hand sides) before gaining
benefits from these hardware features. In the bigger
picture of iterative methods, adaptive block-Jacobi preconditioning is only one
example of recent research in low-precision methods. In addition to the
well-known mixed precision iterative refinement~\cite{higham-ir}, other examples
include the adaptive versions of the Jacobi and PageRank relaxation
methods~\cite{jacobi,jacobi-modular,pagerank} and the development of the modular
precision storage format, which enables efficient access to the same array of
floating point values in multiple
precisions~\cite{jacobi-modular,pagerank,anzt-ir}. It is also worth mentioning
that, orthogonal to the developments in unconventional storage formats, the
potential of performing computations in such formats is also being explored.
While this research rarely results in performance improvements on conventional
hardware, it has the potential to influence the design of future hardware by
demonstrating its effectiveness on simulated systems~\cite{floatx, flexfloat}.

\noindent\textbf{Scientific software.}
As a parting note, the final paragraph of this work focuses on the role of
scientific software in high performance computing. Leaving aside the question of
whether or not ``scientific software engineering'' should be considered an
academic field, it is undeniable that highly efficient software is one of the
most important pillars in modern science based on computer simulations, data
analytics, and machine learning.  Thus, it is not surprising that significant
efforts (including this work) are dedicated to the development of novel
algorithms and their efficient realization on the latest hardware technology.
Paradoxically, even though software is the
most valuable contribution of such efforts, researchers' efficiency is still
is the most valuable contribution to such efforts, a scientist's reputation is
still mostly determined by the traditional metrics like the number of
publications or the Hirsch index. Since the vast majority of journals and
conferences does not have any software quality assurance policies in place, most
scientific software produced today is developed as a prototype ``throwaway''
implementation, intended only to support the publication of a scientific paper,
instead of providing benefits to the broader community. As such, this software
is usually of low quality, poorly documented, and lacks a community which would
maintain it. Furthermore, using such software as a basis for performance
evaluation in a scientific paper is highly questionable, as it usually does not
have to implement edge-case handling, nor takes into account the tradeoffs
between performance gains and the manpower cost of its development, maintenance
and integration into larger projects. The aspect of reproducibility --- one of
the central pillars of science --- is in many cases also ignored. To avoid these
issues, most of the code presented in this work (specifically, the CSR and COO
SpMV algorithms, and the inversion based block-Jacobi and adaptive block-Jacobi
algorithms) are integrated into the Ginkgo software library~\cite{ginkgo},
ensuring continuing community support and maintenance in the future. On
a brighter note, as hardware and software becomes more complex, and the
available manpower scarcer, recent years have witnessed the emergence of
community efforts with the goal of increasing the quality of academic
software~\cite{toms,xsdk,bssw,patch-contrib}.  Hopefully, this trend will
continue, and ultimately lead to a change of metrics that define the success of
a researcher in a direction favourable for scientific software and the high
performance community.
